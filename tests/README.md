# Create a number of s3 buckets for testing purposes

```bash
tests
├── README.md
└── terraform
    ├── README.md
    ├── dev
    │   └── buckets.tfvars
    ├── main.tf
    ├── n
    ├── np
    │   └── buckets.tfvars
    ├── outputs.tf
    ├── prod
    │   └── buckets.tfvars
    ├── staging
    │   └── buckets.tfvars
    ├── terraform.tfstate
    ├── terraform.tfstate.backup
    ├── tf.plan
    ├── tfplan.ansi
    └── variables.tf

6 directories, 14 files

```

# Stress testing the ipaas_cross_account_iam_role function

In order to be sure that the lambda code will successfully create multiple policy documents as intended, we should
 force the code to create multiple read or write policies.

If successful, we should see multiple policies attached to the `dis-s3-bucket-cross-account-access` role in the target
 account, created with a suffix **-1**, **-2** etc

Example:
```bash
dis-managed-ipaas-write-buckets-policy-1
dis-managed-ipaas-write-buckets-policy-2
dis-managed-ipaas-write-objects-policy-1
dis-managed-ipaas-write-objects-policy-2
```

To set up a test, cd to the terraform directory, and modify buckets.tfvars to select the number of test s3 buckets
 to be created, and chose if you want the `ipaas_transfer_enabled` tag to be set to **read** or **write**.

Authenticate to the target AWS account (where you have the dis-ipaas_cross-account-function deployed), which is also
where you will deploy your multitude of stress-test s3 buckets, using either `aws configure sso`, or using the environment
vars from the AWS console

Then follow the terraform steps described below - state & lock files are currently local to this directory

Include a tfvars file to override the defaults specified in variables.tf

Foe example in your tf plan command include `-var-file=dev/buckets.tfvars` to set the number of s3 buckets to be used
for stress testing in the dev or staging environments
```bash
cd ipaas_cross_account_iam_role/tests/terraform

aws s3 ls  | wc -l              #  quick visual check - how many s3 buckets already exist?
terrafom init
terraform plan -var-file=dev/buckets.tfvars -out=tf.plan

tfplan                          # review the planned changes
terraform apply tf.plan
```

**!!! Remember to destroy your s3 test buckets once testing has been completed !!!**

```bash
terraform destroy
```

## TFPlan
In order to make it easier to review the TF plan, or to supply the plan as an artifact in a commit, or a change request,
 I have set up a simple command alias

I also add a switch to the `terraform plan` command to save a copy of the plan in plain text (which the tfplan alias
then converts to a file containing ascii colour coding)

The plan file is called `tf.plan`

```bash
# add this command alias interactively, or to your ~/.bashrc or ~/.zshrc file
alias tfplan='terraform show  tf.plan > tfplan.ansi && less tfplan.ansi'
```

Then run your terraform commands, like this
```bash
terraform init
terraform plan -var-file=buckets.tfvars -out=tf.plan
```

to review the plan, just type the command `tfplan` - then you can apply the plan
```bash
# review
tfplan

# apply
terraform apply tf.plan
```

# After Testing - destroy all the test buckets generated by this terraform config
```bash
terraform destroy
...
...
Plan: 0 to add, 0 to change, 165 to destroy.

Do you really want to destroy all resources?
  Terraform will destroy all your managed infrastructure, as shown above.
  There is no undo. Only 'yes' will be accepted to confirm.

  Enter a value:
```

## Pre-commit githooks
This project also has pre-commit hooks configured. To use them, simply install the pre-commit package
by following the instructions here - https://pre-commit.com/

To activate pre-commit in your project, cd to the local project root, and run `pre-commit install` to set up the
githooks in your repository

```bash
pip3 install pre-commit
pre-commit install
```

To test it ...
```bash
pre-commit run --all-files
```
## Yor - infrastructure tags

Yor tagging (from bridgecrew, available at https://yor.io/) has been run manually (ideally this should be run as part
 of a CI/CD pipeline on every deployment)

This is the command to be used (hint: run from the project root directory, to catch all deployable objects)
```bash
yor tag -d .
```

This will update any terraform blocks that will deploy objects to the cloud - add & commit the updated files, then
 deploy the updated tags to your infrastructure
